{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table of content\n",
    "1) [Load stats](#load-stats)\n",
    "2) [Show histograms and barplots](#show-histograms-and-barplots)\n",
    "3) [Pie on heights](#pie-on-heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and general utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import open3d as o3d\n",
    "import laspy\n",
    "import pdal\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_las_to_laz(in_las, out_laz, verbose=True):\n",
    "    \"\"\"\n",
    "    Convert a LAS file to a LAZ file, stripping all extra dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    - in_las: str, path to the input .las file\n",
    "    - out_laz: str, path to the output .laz file\n",
    "    - verbose: bool, whether to print a success message\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": in_las\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"filename\": out_laz,\n",
    "                \"compression\": \"laszip\",  # Ensure compression to LAZ\n",
    "                # \"extra_dims\": \"none\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create and execute the pipeline\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "    pipeline.execute()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"LAZ file saved at {out_laz}\")\n",
    "\n",
    "def convert_pcd_to_laz(in_pcd, out_laz, verbose=True):\n",
    "    # pcd = laspy.read('../data/testing_samples/split_0332.pcd')\n",
    "    pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            in_pcd,  # Read the PCD file\n",
    "            {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"filename\": out_laz,\n",
    "                \"compression\": \"laszip\"  # Ensures .laz compression\n",
    "                \"\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.reprojection\",\n",
    "                \"in_srs\": \"EPSG:4326\",\n",
    "                \"out_srs\": \"EPSG:2056\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Run the PDAL pipeline\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "    pipeline.execute()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"LAZ file saved in {out_laz}\")\n",
    "\n",
    "def convert_laz_to_pcd(in_laz, out_pcd, verbose=True):\n",
    "    laz = laspy.read(in_laz)\n",
    "\n",
    "    # Gathering all attributes from laz file\n",
    "    points = np.vstack((laz.x, laz.y, laz.z)).T\n",
    "\n",
    "    attributes = {}\n",
    "    for attribute in laz.point_format.dimensions:\n",
    "        if attribute.name in ['X', 'Y', 'Z']:\n",
    "            continue\n",
    "        attributes[attribute.name] = getattr(laz, attribute.name)\n",
    "    \n",
    "    # Preparing data for pcd\n",
    "    num_points = points.shape[0]\n",
    "    fields = [\"x\", \"y\", \"z\"] + list(attributes.keys())  # All field names\n",
    "    types = [\"F\", \"F\", \"F\"] + [\"F\" for _ in attributes]  # Float32 fields\n",
    "    sizes = [4] * len(fields)  # 4-byte float per field\n",
    "\n",
    "    # Stack all data into a single NumPy array\n",
    "    data = np.column_stack([points] + [attributes[key] for key in attributes])\n",
    "\n",
    "    # Write to a PCD file\n",
    "    with open(out_pcd, \"w\") as f:\n",
    "        # f.write(f\"# .PCD v0.7 - Point Cloud Data file format\\n\")\n",
    "        f.write(f\"VERSION 0.7\\n\")\n",
    "        f.write(f\"FIELDS {' '.join(fields)}\\n\")\n",
    "        f.write(f\"SIZE {' '.join(map(str, sizes))}\\n\")\n",
    "        f.write(f\"TYPE {' '.join(types)}\\n\")\n",
    "        f.write(f\"COUNT {' '.join(['1'] * len(fields))}\\n\")\n",
    "        f.write(f\"WIDTH {num_points}\\n\")\n",
    "        f.write(f\"HEIGHT 1\\n\")\n",
    "        f.write(f\"VIEWPOINT 0 0 0 1 0 0 0\\n\")\n",
    "        f.write(f\"POINTS {num_points}\\n\")\n",
    "        f.write(f\"DATA ascii\\n\")\n",
    "    \n",
    "        # Write data\n",
    "        np.savetxt(f, data, fmt=\" \".join([\"%.6f\"] * len(fields)))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"PCD file saved in {out_pcd}\")\n",
    "\n",
    "\n",
    "# convert_pcd_to_laz(r\"C:\\temp_stockage_pdm\\PDM_repos\\Data_samples_cat\\Single\\color_grp_000020.pcd\",r\"C:\\temp_stockage_pdm\\PDM_repos\\Data_samples_cat\\Single\\color_grp_000020.laz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading sources\n",
    "src_folder_instances = r\"D:\\PDM_repo\\Github\\PDM\\data\\full_dataset\\selection\\clusters_4\\cluster_4\\gt\"\n",
    "src_original_prediction = r\"D:\\PDM_repo\\Github\\PDM\\data\\full_dataset\\selection\\clusters_4\\cluster_4\\color_grp_full_tile_331.laz\"\n",
    "src_folder_result = r\"..\\data\\full_dataset\\selection\\clusters_4\\gt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 68.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate from laz to pcd for manual cleaning of the samples\n",
    "files = [x for x in os.listdir(src_folder_instances) if x.endswith('.laz')]\n",
    "src_pcd_loc = os.path.join(src_folder_instances, 'pcd')\n",
    "os.makedirs(src_pcd_loc, exist_ok=True)\n",
    "for _, file in tqdm(enumerate(files), total=len(files)):\n",
    "    file_out = file.split('.laz')[0] + '.pcd'\n",
    "    convert_laz_to_pcd(os.path.join(src_folder_instances, file), os.path.join(src_pcd_loc, file_out), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 32.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Once cleaned, generate from pcd to laz in new folder\n",
    "src_folder_instances = os.path.join(src_folder_instances, 'pcd/modified_samples')\n",
    "files = [x for x in os.listdir(src_folder_instances) if x.endswith('.pcd')]\n",
    "for _, file in tqdm(enumerate(files), total=len(files)):\n",
    "    src_in = os.path.join(src_folder_instances, file)\n",
    "    src_out = os.path.join(src_folder_instances, file.split('.pcd')[0] + '.laz')\n",
    "    convert_pcd_to_laz(src_in, src_out,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original and reset/create gt columns\n",
    "full_tile = laspy.read(src_original_prediction)\n",
    "full_tile.add_extra_dim(laspy.ExtraBytesParams('gt_semantic',type=\"uint16\"))\n",
    "full_tile.add_extra_dim(laspy.ExtraBytesParams('gt_instance',type=\"uint16\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:09<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loop on gt instances and set the correct values in the full tile\n",
    "list_instances_src = [x for x in os.listdir(src_folder_instances) if x.endswith('.laz')]\n",
    "rounding = 2\n",
    "semantic_layer = np.zeros(len(full_tile))\n",
    "instance_layer = np.zeros(len(full_tile))\n",
    "for id_instance, instance_src in tqdm(enumerate(list_instances_src), total=len(list_instances_src)):\n",
    "    instance = laspy.read(os.path.join(src_folder_instances, instance_src))\n",
    "    coords = list(zip(np.round(instance.x, rounding), np.round(instance.y, rounding), np.round(instance.z, rounding)))\n",
    "    mask = np.array([(x,y,z) in coords for x, y, z in zip(np.round(full_tile.x, rounding), np.round(full_tile.y, rounding), np.round(full_tile.z, rounding))])\n",
    "    semantic_layer[mask] = 1\n",
    "    instance_layer[mask] = id_instance + 1\n",
    "    # print(np.sum(mask))\n",
    "    # print(len(coords))\n",
    "    # assert np.sum(mask) == len(coords)\n",
    "\n",
    "setattr(full_tile, 'gt_semantic', semantic_layer)\n",
    "setattr(full_tile, 'gt_instance', instance_layer)\n",
    "\n",
    "# save file\n",
    "new_file = os.path.join(os.path.join(src_folder_result), os.path.basename(src_original_prediction).split('.laz')[0] + '_gt.laz')\n",
    "full_tile.write(new_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading sources\n",
    "src_folder_instances = r\"D:\\PDM_repo\\Github\\PDM\\data\\full_dataset\\selection\\clusters_4\\cluster_2\\gt\\round2\"\n",
    "src_target = r\"D:\\PDM_repo\\Github\\PDM\\data\\full_dataset\\selection\\clusters_4\\gt\\color_grp_full_tile_331_gt.laz\"\n",
    "tile_target = laspy.read(src_target)\n",
    "\n",
    "assert \"gt_semantic\" in tile_target.point_format.dimension_names\n",
    "assert \"gt_instance\" in tile_target.point_format.dimension_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 40.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate from laz to pcd for manual cleaning of the samples\n",
    "files = [x for x in os.listdir(src_folder_instances) if x.endswith('.laz')]\n",
    "src_pcd_loc = os.path.join(src_folder_instances, 'pcd')\n",
    "os.makedirs(src_pcd_loc, exist_ok=True)\n",
    "for _, file in tqdm(enumerate(files), total=len(files)):\n",
    "    file_out = file.split('.laz')[0] + '.pcd'\n",
    "    convert_laz_to_pcd(os.path.join(src_folder_instances, file), os.path.join(src_pcd_loc, file_out), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 30.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Once cleaned, generate from pcd to laz in new folder\n",
    "src_folder_instances = os.path.join(src_folder_instances, 'pcd/modified_samples')\n",
    "files = [x for x in os.listdir(src_folder_instances) if x.endswith('.pcd')]\n",
    "for _, file in tqdm(enumerate(files), total=len(files)):\n",
    "    src_in = os.path.join(src_folder_instances, file)\n",
    "    src_out = os.path.join(src_folder_instances, file.split('.pcd')[0] + '.laz')\n",
    "    convert_pcd_to_laz(src_in, src_out,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:39<00:00, 11.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loop on gt instances and set the correct values in the full tile\n",
    "list_instances_src = [x for x in os.listdir(src_folder_instances) if x.endswith('.laz')]\n",
    "rounding = 2\n",
    "semantic_layer = np.array(tile_target.gt_semantic)\n",
    "instance_layer = np.array(tile_target.gt_instance)\n",
    "# instance_layer = np.zeros(len(tile_target))\n",
    "instance_val = np.max(tile_target.gt_instance) + 1\n",
    "for id_instance, instance_src in tqdm(enumerate(list_instances_src), total=len(list_instances_src)):\n",
    "    instance = laspy.read(os.path.join(src_folder_instances, instance_src))\n",
    "    coords = list(zip(np.round(instance.x, rounding), np.round(instance.y, rounding), np.round(instance.z, rounding)))\n",
    "    mask = np.array([(x,y,z) in coords for x, y, z in zip(np.round(tile_target.x, rounding), np.round(tile_target.y, rounding), np.round(tile_target.z, rounding))])\n",
    "    semantic_layer[mask] = 1\n",
    "    instance_layer[mask] = instance_val\n",
    "    instance_val += 1\n",
    "    # print(np.sum(mask))\n",
    "    # print(len(coords))\n",
    "    # assert np.sum(mask) == len(coords)\n",
    "\n",
    "setattr(tile_target, 'gt_semantic', semantic_layer)\n",
    "setattr(tile_target, 'gt_instance', instance_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save file\n",
    "new_file = os.path.join(os.path.join(src_folder_result), os.path.basename(src_target).split('.laz')[0] + '_2.laz')\n",
    "tile_target.write(new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\\full_dataset\\selection\\clusters_4\\gt\\color_grp_full_tile_331_gt_2.laz\n"
     ]
    }
   ],
   "source": [
    "print(new_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erase clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with id 100 of size 0 deleted\n"
     ]
    }
   ],
   "source": [
    "tree_ids_to_erase = [100]\n",
    "src_tile = r\"D:\\PDM_repo\\Github\\PDM\\data\\full_dataset\\selection\\clusters_4\\gt\\color_grp_full_tile_317_gt.laz\"\n",
    "tile = laspy.read(src_tile)\n",
    "assert \"gt_instance_segmentation\" in list(tile.point_format.dimension_names)\n",
    "\n",
    "for tree_id in tree_ids_to_erase:\n",
    "    mask = tile.gt_instance_segmentation == tree_id\n",
    "    tile.gt_instance_segmentation[mask] = 0.0\n",
    "    # setattr(tile[tile.gt_instance_segmentation == float(tree_id)], 'gt_instance_segmentation', 0.0)\n",
    "    print(f\"Tree with id {tree_id} of size {np.sum(mask)} deleted\")\n",
    "tile.write(src_tile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean ids of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:00<00:00, 615.63it/s]\n"
     ]
    }
   ],
   "source": [
    "src_tile = r\"D:\\PDM_repo\\Github\\PDM\\data\\full_dataset\\selection\\clusters_4\\gt\\color_grp_full_tile_317_gt.laz\"\n",
    "tile = laspy.read(src_tile)\n",
    "assert \"gt_instance_segmentation\" in list(tile.point_format.dimension_names)\n",
    "max_id = np.max(tile.gt_instance_segmentation)\n",
    "down_jump = 0\n",
    "for _, id in tqdm(enumerate(range(max_id+1)), total=max_id+1):\n",
    "    mask = tile.gt_instance_segmentation == id\n",
    "    if np.sum(mask) == 0:\n",
    "        print(f\"Empty id: {id}\")\n",
    "        down_jump += 1\n",
    "        continue\n",
    "\n",
    "    if down_jump > 0:\n",
    "        print(id, \" -> \", id - down_jump)\n",
    "        tile.gt_instance_segmentation[mask] = id - down_jump\n",
    "tile.write(src_tile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change name of semantic and segmentation columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old val:  gt_semantic_segmentation\n",
      "New val:  gt_semantic\n",
      "Old val:  gt_instance_segmentation\n",
      "New val:  gt_instance\n"
     ]
    }
   ],
   "source": [
    "src_tile = r\"D:\\PDM_repo\\Github\\PDM\\data\\gt\\color_grp_full_tile_331_gt.laz\"\n",
    "las = laspy.read(src_tile)\n",
    "# print(las.extra_dimensions)\n",
    "for old_val, new_val in zip(['gt_semantic_segmentation', 'gt_instance_segmentation'],['gt_semantic', 'gt_instance']):\n",
    "    print(\"Old val: \", old_val)\n",
    "    print(\"New val: \", new_val)\n",
    "    # Get the values\n",
    "    values = las[old_val]\n",
    "\n",
    "    # Remove the old dimension from extra dimensions (only works for ExtraBytes dimensions)\n",
    "    if old_val in (las.point_format.dimension_names):\n",
    "        las.remove_extra_dim(old_val)\n",
    "\n",
    "    # Add new dimension\n",
    "    las.add_extra_dim(laspy.ExtraBytesParams(name=new_val, type=np.float32))  # Change type if needed\n",
    "    las[new_val] = values\n",
    "\n",
    "# Save to new file\n",
    "las.write(src_tile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
