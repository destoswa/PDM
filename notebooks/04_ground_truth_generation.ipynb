{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table of content\n",
    "1) [Load stats](#load-stats)\n",
    "2) [Show histograms and barplots](#show-histograms-and-barplots)\n",
    "3) [Pie on heights](#pie-on-heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and general utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import open3d as o3d\n",
    "import laspy\n",
    "import pdal\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_las_to_laz(in_las, out_laz, verbose=True):\n",
    "    \"\"\"\n",
    "    Convert a LAS file to a LAZ file, stripping all extra dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    - in_las: str, path to the input .las file\n",
    "    - out_laz: str, path to the output .laz file\n",
    "    - verbose: bool, whether to print a success message\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": in_las\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"filename\": out_laz,\n",
    "                \"compression\": \"laszip\",  # Ensure compression to LAZ\n",
    "                # \"extra_dims\": \"none\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create and execute the pipeline\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "    pipeline.execute()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"LAZ file saved at {out_laz}\")\n",
    "\n",
    "def convert_pcd_to_laz(in_pcd, out_laz, verbose=True):\n",
    "    # pcd = laspy.read('../data/testing_samples/split_0332.pcd')\n",
    "    pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            in_pcd,  # Read the PCD file\n",
    "            {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"filename\": out_laz,\n",
    "                \"compression\": \"laszip\"  # Ensures .laz compression\n",
    "                \"\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.reprojection\",\n",
    "                \"in_srs\": \"EPSG:4326\",\n",
    "                \"out_srs\": \"EPSG:2056\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Run the PDAL pipeline\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_json))\n",
    "    pipeline.execute()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"LAZ file saved in {out_laz}\")\n",
    "\n",
    "def convert_laz_to_pcd(in_laz, out_pcd, verbose=True):\n",
    "    laz = laspy.read(in_laz)\n",
    "\n",
    "    # Gathering all attributes from laz file\n",
    "    points = np.vstack((laz.x, laz.y, laz.z)).T\n",
    "\n",
    "    attributes = {}\n",
    "    for attribute in laz.point_format.dimensions:\n",
    "        if attribute.name in ['X', 'Y', 'Z']:\n",
    "            continue\n",
    "        attributes[attribute.name] = getattr(laz, attribute.name)\n",
    "    \n",
    "    # Preparing data for pcd\n",
    "    num_points = points.shape[0]\n",
    "    fields = [\"x\", \"y\", \"z\"] + list(attributes.keys())  # All field names\n",
    "    types = [\"F\", \"F\", \"F\"] + [\"F\" for _ in attributes]  # Float32 fields\n",
    "    sizes = [4] * len(fields)  # 4-byte float per field\n",
    "\n",
    "    # Stack all data into a single NumPy array\n",
    "    data = np.column_stack([points] + [attributes[key] for key in attributes])\n",
    "\n",
    "    # Write to a PCD file\n",
    "    with open(out_pcd, \"w\") as f:\n",
    "        # f.write(f\"# .PCD v0.7 - Point Cloud Data file format\\n\")\n",
    "        f.write(f\"VERSION 0.7\\n\")\n",
    "        f.write(f\"FIELDS {' '.join(fields)}\\n\")\n",
    "        f.write(f\"SIZE {' '.join(map(str, sizes))}\\n\")\n",
    "        f.write(f\"TYPE {' '.join(types)}\\n\")\n",
    "        f.write(f\"COUNT {' '.join(['1'] * len(fields))}\\n\")\n",
    "        f.write(f\"WIDTH {num_points}\\n\")\n",
    "        f.write(f\"HEIGHT 1\\n\")\n",
    "        f.write(f\"VIEWPOINT 0 0 0 1 0 0 0\\n\")\n",
    "        f.write(f\"POINTS {num_points}\\n\")\n",
    "        f.write(f\"DATA ascii\\n\")\n",
    "    \n",
    "        # Write data\n",
    "        np.savetxt(f, data, fmt=\" \".join([\"%.6f\"] * len(fields)))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"PCD file saved in {out_pcd}\")\n",
    "\n",
    "\n",
    "# convert_pcd_to_laz(r\"C:\\temp_stockage_pdm\\PDM_repos\\Data_samples_cat\\Single\\color_grp_000020.pcd\",r\"C:\\temp_stockage_pdm\\PDM_repos\\Data_samples_cat\\Single\\color_grp_000020.laz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading sources\n",
    "src_folder_instances = r\"D:\\PDM_repo\\Github\\PDM\\data\\full_dataset\\selection\\clusters_4\\cluster_4\\gt\"\n",
    "src_original_prediction = r\"D:\\PDM_repo\\Github\\PDM\\data\\full_dataset\\selection\\clusters_4\\cluster_4\\color_grp_full_tile_331.laz\"\n",
    "src_folder_result = r\"..\\data\\full_dataset\\selection\\clusters_4\\gt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 68.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate from laz to pcd for manual cleaning of the samples\n",
    "files = [x for x in os.listdir(src_folder_instances) if x.endswith('.laz')]\n",
    "src_pcd_loc = os.path.join(src_folder_instances, 'pcd')\n",
    "os.makedirs(src_pcd_loc, exist_ok=True)\n",
    "for _, file in tqdm(enumerate(files), total=len(files)):\n",
    "    file_out = file.split('.laz')[0] + '.pcd'\n",
    "    convert_laz_to_pcd(os.path.join(src_folder_instances, file), os.path.join(src_pcd_loc, file_out), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 32.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Once cleaned, generate from pcd to laz in new folder\n",
    "src_folder_instances = os.path.join(src_folder_instances, 'pcd/modified_samples')\n",
    "files = [x for x in os.listdir(src_folder_instances) if x.endswith('.pcd')]\n",
    "for _, file in tqdm(enumerate(files), total=len(files)):\n",
    "    src_in = os.path.join(src_folder_instances, file)\n",
    "    src_out = os.path.join(src_folder_instances, file.split('.pcd')[0] + '.laz')\n",
    "    convert_pcd_to_laz(src_in, src_out,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original and reset/create gt columns\n",
    "full_tile = laspy.read(src_original_prediction)\n",
    "full_tile.add_extra_dim(laspy.ExtraBytesParams('gt_semantic_segmentation',type=\"uint16\"))\n",
    "full_tile.add_extra_dim(laspy.ExtraBytesParams('gt_instance_segmentation',type=\"uint16\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:09<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loop on gt instances and set the correct values in the full tile\n",
    "list_instances_src = [x for x in os.listdir(src_folder_instances) if x.endswith('.laz')]\n",
    "rounding = 2\n",
    "semantic_layer = np.zeros(len(full_tile))\n",
    "instance_layer = np.zeros(len(full_tile))\n",
    "for id_instance, instance_src in tqdm(enumerate(list_instances_src), total=len(list_instances_src)):\n",
    "    instance = laspy.read(os.path.join(src_folder_instances, instance_src))\n",
    "    coords = list(zip(np.round(instance.x, rounding), np.round(instance.y, rounding), np.round(instance.z, rounding)))\n",
    "    mask = np.array([(x,y,z) in coords for x, y, z in zip(np.round(full_tile.x, rounding), np.round(full_tile.y, rounding), np.round(full_tile.z, rounding))])\n",
    "    semantic_layer[mask] = 1\n",
    "    instance_layer[mask] = id_instance + 1\n",
    "    # print(np.sum(mask))\n",
    "    # print(len(coords))\n",
    "    # assert np.sum(mask) == len(coords)\n",
    "\n",
    "setattr(full_tile, 'gt_semantic_segmentation', semantic_layer)\n",
    "setattr(full_tile, 'gt_instance_segmentation', instance_layer)\n",
    "\n",
    "# save file\n",
    "new_file = os.path.join(os.path.join(src_folder_result), os.path.basename(src_original_prediction).split('.laz')[0] + '_gt.laz')\n",
    "full_tile.write(new_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading sources\n",
    "src_folder_instances = r\"D:\\PDM_repo\\Github\\PDM\\data\\full_dataset\\selection\\clusters_4\\cluster_2\\gt\\round2\"\n",
    "src_target = r\"D:\\PDM_repo\\Github\\PDM\\data\\full_dataset\\selection\\clusters_4\\gt\\color_grp_full_tile_331_gt.laz\"\n",
    "tile_target = laspy.read(src_target)\n",
    "\n",
    "assert \"gt_semantic_segmentation\" in tile_target.point_format.dimension_names\n",
    "assert \"gt_instance_segmentation\" in tile_target.point_format.dimension_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 40.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate from laz to pcd for manual cleaning of the samples\n",
    "files = [x for x in os.listdir(src_folder_instances) if x.endswith('.laz')]\n",
    "src_pcd_loc = os.path.join(src_folder_instances, 'pcd')\n",
    "os.makedirs(src_pcd_loc, exist_ok=True)\n",
    "for _, file in tqdm(enumerate(files), total=len(files)):\n",
    "    file_out = file.split('.laz')[0] + '.pcd'\n",
    "    convert_laz_to_pcd(os.path.join(src_folder_instances, file), os.path.join(src_pcd_loc, file_out), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 30.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Once cleaned, generate from pcd to laz in new folder\n",
    "src_folder_instances = os.path.join(src_folder_instances, 'pcd/modified_samples')\n",
    "files = [x for x in os.listdir(src_folder_instances) if x.endswith('.pcd')]\n",
    "for _, file in tqdm(enumerate(files), total=len(files)):\n",
    "    src_in = os.path.join(src_folder_instances, file)\n",
    "    src_out = os.path.join(src_folder_instances, file.split('.pcd')[0] + '.laz')\n",
    "    convert_pcd_to_laz(src_in, src_out,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:39<00:00, 11.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loop on gt instances and set the correct values in the full tile\n",
    "list_instances_src = [x for x in os.listdir(src_folder_instances) if x.endswith('.laz')]\n",
    "rounding = 2\n",
    "semantic_layer = np.array(tile_target.gt_semantic_segmentation)\n",
    "instance_layer = np.array(tile_target.gt_instance_segmentation)\n",
    "# instance_layer = np.zeros(len(tile_target))\n",
    "instance_val = np.max(tile_target.gt_instance_segmentation) + 1\n",
    "for id_instance, instance_src in tqdm(enumerate(list_instances_src), total=len(list_instances_src)):\n",
    "    instance = laspy.read(os.path.join(src_folder_instances, instance_src))\n",
    "    coords = list(zip(np.round(instance.x, rounding), np.round(instance.y, rounding), np.round(instance.z, rounding)))\n",
    "    mask = np.array([(x,y,z) in coords for x, y, z in zip(np.round(tile_target.x, rounding), np.round(tile_target.y, rounding), np.round(tile_target.z, rounding))])\n",
    "    semantic_layer[mask] = 1\n",
    "    instance_layer[mask] = instance_val\n",
    "    instance_val += 1\n",
    "    # print(np.sum(mask))\n",
    "    # print(len(coords))\n",
    "    # assert np.sum(mask) == len(coords)\n",
    "\n",
    "setattr(tile_target, 'gt_semantic_segmentation', semantic_layer)\n",
    "setattr(tile_target, 'gt_instance_segmentation', instance_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save file\n",
    "new_file = os.path.join(os.path.join(src_folder_result), os.path.basename(src_target).split('.laz')[0] + '_2.laz')\n",
    "tile_target.write(new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\\full_dataset\\selection\\clusters_4\\gt\\color_grp_full_tile_331_gt_2.laz\n"
     ]
    }
   ],
   "source": [
    "print(new_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erase clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with id 100 of size 0 deleted\n"
     ]
    }
   ],
   "source": [
    "tree_ids_to_erase = [100]\n",
    "src_tile = r\"D:\\PDM_repo\\Github\\PDM\\data\\full_dataset\\selection\\clusters_4\\gt\\color_grp_full_tile_317_gt.laz\"\n",
    "tile = laspy.read(src_tile)\n",
    "assert \"gt_instance_segmentation\" in list(tile.point_format.dimension_names)\n",
    "\n",
    "for tree_id in tree_ids_to_erase:\n",
    "    mask = tile.gt_instance_segmentation == tree_id\n",
    "    tile.gt_instance_segmentation[mask] = 0.0\n",
    "    # setattr(tile[tile.gt_instance_segmentation == float(tree_id)], 'gt_instance_segmentation', 0.0)\n",
    "    print(f\"Tree with id {tree_id} of size {np.sum(mask)} deleted\")\n",
    "tile.write(src_tile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean ids of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:00<00:00, 615.63it/s]\n"
     ]
    }
   ],
   "source": [
    "src_tile = r\"D:\\PDM_repo\\Github\\PDM\\data\\full_dataset\\selection\\clusters_4\\gt\\color_grp_full_tile_317_gt.laz\"\n",
    "tile = laspy.read(src_tile)\n",
    "assert \"gt_instance_segmentation\" in list(tile.point_format.dimension_names)\n",
    "max_id = np.max(tile.gt_instance_segmentation)\n",
    "down_jump = 0\n",
    "for _, id in tqdm(enumerate(range(max_id+1)), total=max_id+1):\n",
    "    mask = tile.gt_instance_segmentation == id\n",
    "    if np.sum(mask) == 0:\n",
    "        print(f\"Empty id: {id}\")\n",
    "        down_jump += 1\n",
    "        continue\n",
    "\n",
    "    if down_jump > 0:\n",
    "        print(id, \" -> \", id - down_jump)\n",
    "        tile.gt_instance_segmentation[mask] = id - down_jump\n",
    "tile.write(src_tile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic results:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "LasData object has no attribute 'gt_semantic_segmentation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# verify results and save file\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSemantic results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[43mfull_tile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgt_semantic_segmentation\u001b[49m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mVal \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: number of points = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(full_tile\u001b[38;5;241m.\u001b[39mgt_semantic_segmentation[full_tile\u001b[38;5;241m.\u001b[39mgt_semantic_segmentation\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39mcat])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstances results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\PDM\\lib\\site-packages\\laspy\\lasdata.py:378\u001b[0m, in \u001b[0;36mLasData.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints[item]\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    380\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: LasData object has no attribute 'gt_semantic_segmentation'"
     ]
    }
   ],
   "source": [
    "# verify results and save file\n",
    "print(\"Semantic results:\")\n",
    "for cat in set(full_tile.gt_semantic_segmentation):\n",
    "    print(f\"\\tVal {cat}: number of points = {len(full_tile.gt_semantic_segmentation[full_tile.gt_semantic_segmentation == cat])}\")\n",
    "print(\"Instances results:\")\n",
    "for cat in set(full_tile.gt_instance_segmentation):\n",
    "    print(f\"\\tVal {cat}: number of points = {len(full_tile.gt_instance_segmentation[full_tile.gt_instance_segmentation == cat])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "Y\n",
      "Z\n",
      "intensity\n",
      "return_number\n",
      "number_of_returns\n",
      "synthetic\n",
      "key_point\n",
      "withheld\n",
      "overlap\n",
      "scanner_channel\n",
      "scan_direction_flag\n",
      "edge_of_flight_line\n",
      "classification\n",
      "user_data\n",
      "scan_angle\n",
      "point_source_id\n",
      "gps_time\n",
      "red\n",
      "green\n",
      "blue\n",
      "PredSemantic\n",
      "gt_semantic_segmentation\n",
      "PredInstance\n",
      "gt_instance_segmentation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for col in list(full_tile.point_format.dimension_names):\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing clean samples into original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_origina"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
