{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### table of content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and general utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import open3d as o3d\n",
    "import laspy\n",
    "import pdal\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import cKDTree\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_preds = r\"..\\data\\flattening_corrections\\predictions\"\n",
    "src_gt = r\"..\\data\\flattening_corrections\\gt\"\n",
    "src_floors = r\"..\\data\\flattening_corrections\\floors\"\n",
    "src_masks = r\"..\\data\\flattening_corrections\\masks\"\n",
    "src_originals = r\"..\\data\\flattening_corrections\\originals\"\n",
    "src_flatten = r\"..\\data\\flattening_corrections\\flatten\"\n",
    "src_results = r\"..\\data\\flattening_corrections\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_preds = {}\n",
    "list_masks = {}\n",
    "list_floors = {}\n",
    "list_flatten = {}\n",
    "list_floors = {}\n",
    "list_gt = {}\n",
    "list_originals = {}\n",
    "\n",
    "samples_num = [128, 129, 160, 210, 311, 633]\n",
    "tilling_num = [1, 5, 10, 20]\n",
    "# for num in samples_num:\n",
    "#     list_preds[num] = []\n",
    "#     list_masks[num] = []\n",
    "#     list_floors[num] = []\n",
    "#     list_originals[num] = []\n",
    "\n",
    "for r, _, f in os.walk(src_preds):\n",
    "    for num in samples_num:\n",
    "        list_preds[num] = [os.path.join(r, file) for file in f if len(file.split(str(num))) > 1]\n",
    "for r, _, f in os.walk(src_masks):\n",
    "    for num in samples_num:\n",
    "        list_masks[num] = [os.path.join(r, file) for file in f if len(file.split(str(num))) > 1]\n",
    "for r, _, f in os.walk(src_floors):\n",
    "    for num in samples_num:\n",
    "        list_floors[num] = [os.path.join(r, file) for file in f if len(file.split(str(num))) > 1]\n",
    "for r, _, f in os.walk(src_flatten):\n",
    "    for num in samples_num:\n",
    "        list_flatten[num] = [os.path.join(r, file) for file in f if len(file.split(str(num))) > 1]\n",
    "for r, _, f in os.walk(src_originals):\n",
    "    for num in samples_num:\n",
    "        list_originals[num] = [os.path.join(r, file) for file in f if len(file.split(str(num))) > 1][0]\n",
    "for r, _, f in os.walk(src_gt):\n",
    "    for num in samples_num:\n",
    "        list_gt[num] = [os.path.join(r, file) for file in f if len(file.split(str(num))) > 1][0]\n",
    "# print(list_gt)\n",
    "# print(list_preds)\n",
    "\n",
    "#     for file in f:\n",
    "#         list_preds[os.join.path(r, file)] = []\n",
    "#         list_masks[os.join.path(r, file)] = []\n",
    "#         list_floors[os.join.path(r, file)] = []\n",
    "#         list_originals[os.join.path(r, file)] = []\n",
    "\n",
    "# for r, _, f in os.walk(src_preds):\n",
    "#     for file in f:\n",
    "\n",
    "#         # list_originals.append(os.path.join(r,file))\n",
    "# for r, _, f in os.walk(src_gt):\n",
    "#     for file in f:\n",
    "#         list_preds[os.path.join(r, file)] = []\n",
    "#         list_masks_gt[os.path.join(r, file)] = []\n",
    "#         list_floors_gt[os.path.join(r, file)] = []\n",
    "#         # list_originals[os.path.join(r, file)] = []\n",
    "#         for r_pred, _, f_pred in os.walk(src_preds):\n",
    "#             for file_pred in f_pred:\n",
    "#                 if file_pred.split('_flatten')[0].split('_out')[0] == file.split(\"_out_gt.laz\")[0]:\n",
    "#                     list_preds[os.path.join(r, file)].append(os.path.join(r_pred, file_pred))\n",
    "#         for r_mask, _, f_mask in os.walk(src_masks):\n",
    "#             for file_mask in f_mask:\n",
    "#                 if file_mask.split('_mask')[0].split('_out')[0] == file.split(\"_out_gt.laz\")[0]:\n",
    "#                     list_masks_gt[os.path.join(r, file)].append(os.path.join(r_mask, file_mask))\n",
    "#         for r_floor, d_floor, f_floor in os.walk(src_floors):\n",
    "#             for file_floor in f_floor:\n",
    "#                 if file_floor.split('_floor')[0].split('_out')[0] == file.split(\"_out_gt.laz\")[0]:\n",
    "#                     list_floors_gt[os.path.join(r, file)].append(os.path.join(r_floor, file_floor))\n",
    "#         # for r_original, _, f_original in os.walk(src_originals):\n",
    "#         #     for file_original in f_original:\n",
    "#         #         if file_original.split('.laz')[0].split('_flatten')[0] == file.split(\"_out_gt.laz\")[0]:\n",
    "#         #             list_originals[os.path.join(r, file)].append(os.path.join(r_original, file_original))\n",
    "\n",
    "# for r, d, f in os.walk(src_gt):\n",
    "#     for file in f:\n",
    "#         list_floors.append(os.path.join(r, file))\n",
    "# print(list_floors_gt)\n",
    "# print(list_originals)\n",
    "# for (key, values), (key_2, mask), (key_3, floor) in zip(list_preds_gt.items(), list_masks_gt.items(), list_floors_gt.items()):\n",
    "# # for key, values in list_preds_gt.items():\n",
    "#     print(key,\":\")\n",
    "#     for pred in values:\n",
    "#         print(\"\\t\", pred)\n",
    "#     print('\\t---')\n",
    "#     for m in mask:\n",
    "#         print(\"\\t\", m)\n",
    "#     print('\\t---')\n",
    "#     for f in floor:\n",
    "#         print(\"\\t\", f)\n",
    "#     print('\\t---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_panoptic_quality(gt_instances, pred_instances):\n",
    "    \"\"\"\n",
    "    Computes Panoptic Quality (PQ), Segmentation Quality (SQ), and Recognition Quality (RQ).\n",
    "    \n",
    "    :param gt_instances: List of sets, each containing point indices for a ground truth instance.\n",
    "    :param pred_instances: List of sets, each containing point indices for a predicted instance.\n",
    "    :return: PQ, SQ, RQ\n",
    "    \"\"\"\n",
    "\n",
    "    # gt_instances, pred_instances = get_segmentation(gt_instances, pred_instances)\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    iou_sum = 0\n",
    "\n",
    "    # Match predicted instances to ground truth instances\n",
    "    matched_gt = set()\n",
    "    matched_pred = set()\n",
    "    \n",
    "    for i, gt in enumerate(gt_instances):\n",
    "        best_iou = 0\n",
    "        best_pred = None\n",
    "\n",
    "        for j, pred in enumerate(pred_instances):\n",
    "            iou = len(gt & pred) / len(gt | pred)  # IoU computation\n",
    "            \n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_pred = j\n",
    "        \n",
    "        # Threshold for a valid match\n",
    "        if best_iou > 0.5:\n",
    "            matched_gt.add(i)\n",
    "            matched_pred.add(best_pred)\n",
    "            tp += 1\n",
    "            iou_sum += best_iou\n",
    "        else:\n",
    "            fn += 1  # Unmatched ground truth instance\n",
    "    \n",
    "    fp = len(pred_instances) - len(matched_pred)  # Unmatched predictions\n",
    "\n",
    "    RQ = tp / (tp + 0.5 * (fp + fn)) if (tp + 0.5 * (fp + fn)) > 0 else 0\n",
    "    SQ = iou_sum / tp if tp > 0 else 0\n",
    "    PQ = SQ * RQ\n",
    "\n",
    "    return PQ, SQ, RQ, tp, fp, fn\n",
    "\n",
    "\n",
    "def compute_mean_iou(y_true, y_pred, num_classes=2):\n",
    "    \"\"\"\n",
    "    Computes mean Intersection over Union (mIoU).\n",
    "    \n",
    "    :param y_true: Ground truth labels (N,)\n",
    "    :param y_pred: Predicted labels (N,)\n",
    "    :param num_classes: Total number of classes\n",
    "    :return: Mean IoU score\n",
    "    \"\"\"\n",
    "    iou_list = []\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        tp = np.sum((y_true == c) & (y_pred == c))\n",
    "        fp = np.sum((y_true != c) & (y_pred == c))\n",
    "        fn = np.sum((y_true == c) & (y_pred != c))\n",
    "        \n",
    "        iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "        iou_list.append(iou)\n",
    "\n",
    "    return np.mean(iou_list)\n",
    "\n",
    "\n",
    "def get_segmentation(instance_list, semantic_list):\n",
    "    instances_format = []\n",
    "    semantic_format = []\n",
    "    # Computing instances\n",
    "    for instance in set(instance_list):\n",
    "        if instance == 0: continue\n",
    "        list_points = [pos for pos, val in enumerate(instance_list) if val == instance]\n",
    "        instances_format.append(set(list_points))\n",
    "\n",
    "    # Computing semantic\n",
    "    for semantic in set(semantic_list):\n",
    "        list_points = [pos for pos, val in enumerate(semantic_list) if val == semantic]\n",
    "        semantic_format.append(set(list_points))\n",
    "\n",
    "    return instances_format, semantic_format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_duplicates(laz_file):\n",
    "    # Find pairs of points\n",
    "    coords = np.round(np.vstack((laz_file.x, laz_file.y, laz_file.z)),2).T\n",
    "    tree_B = cKDTree(coords)\n",
    "    pairs = tree_B.query_pairs(1e-2)\n",
    "\n",
    "    # Create the mask with dupplicates\n",
    "    mask = [True for i in range(len(coords))]\n",
    "    for pair in pairs:\n",
    "        mask[pair[1]] = False\n",
    "\n",
    "    # Remove the dupplicates from the file\n",
    "    laz_file.points = laz_file.points[mask]\n",
    "\n",
    "    # # Copy all other attributes (optional)\n",
    "    # for dim in laz_file.point_format.dimensions:\n",
    "    #     if dim.name not in [\"X\", \"Y\", \"Z\"]:  # Skip coordinates\n",
    "    #         setattr(laz_file, dim.name, getattr(laz_file, dim.name)[mask])\n",
    "\n",
    "def match_pointclouds(laz1, laz2):\n",
    "    # Retrieve points\n",
    "    coords_1 = np.round(np.vstack((laz1.x, laz1.y, laz1.z)),2).T\n",
    "    coords_2 = np.round(np.vstack((laz2.x, laz2.y, laz2.z)),2).T\n",
    "\n",
    "    # Verify that lengths match:\n",
    "    assert coords_1.shape == coords_2.shape\n",
    "\n",
    "    # Convert rows to tuples for sorting\n",
    "    coords_1_view = np.ascontiguousarray(coords_1).view([('', coords_1.dtype)] * 3)\n",
    "    coords_2_view = np.ascontiguousarray(coords_2).view([('', coords_2.dtype)] * 3)\n",
    "\n",
    "    # Sort indices\n",
    "    coords_1_sort_idx = np.argsort(coords_1_view.ravel())\n",
    "    coords_2_sort_idx = np.argsort(coords_2_view.ravel())\n",
    "\n",
    "    # Find corresponding indices\n",
    "    coords_1_sorted = coords_1_view.ravel()[coords_1_sort_idx]\n",
    "    coords_2_sorted = coords_2_view.ravel()[coords_2_sort_idx]\n",
    "\n",
    "    # Get matching indices\n",
    "    matching_coords_2_idx = np.searchsorted(coords_2_sorted, coords_1_sorted)\n",
    "\n",
    "    # Restore original indices\n",
    "    matching_coords_1_idx = coords_1_sort_idx\n",
    "    matching_coords_2_idx = coords_2_sort_idx[matching_coords_2_idx]\n",
    "    \n",
    "    # Assign alligned points to laz files\n",
    "    # laz1.xyz = coords_1[matching_coords_1_idx]\n",
    "    # laz2.xyz = coords_2[matching_coords_2_idx]\n",
    "    # laz1.points = laz1.points[matching_coords_1_idx]\n",
    "    # laz2.points = laz2.points[matching_coords_2_idx]\n",
    "    for dim in laz2.point_format.dimensions:  # Skip coordinates\n",
    "        setattr(laz2, dim.name, getattr(laz2, dim.name)[matching_coords_2_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to remove duplicates: 291 ms\n"
     ]
    }
   ],
   "source": [
    "laz_original = laspy.read(r\"..\\data\\flattening_corrections\\originals_old\\color_grp_full_tile_128_flatten_1m.laz\")\n",
    "laz_pred = laspy.read(r\"..\\data\\flattening_corrections\\predictions\\color_grp_full_tile_128_flatten_1m_out.laz\")\n",
    "start = time()\n",
    "remove_duplicates(laz_original)\n",
    "print(f\"Time to remove duplicates: {int(round(time() - start, 3) * 1000)} ms\")\n",
    "remove_duplicates(laz_pred)\n",
    "match_pointclouds(laz_original, laz_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'Y', 'Z', 'intensity', 'return_number', 'number_of_returns', 'synthetic', 'key_point', 'withheld', 'overlap', 'scanner_channel', 'scan_direction_flag', 'edge_of_flight_line', 'classification', 'user_data', 'scan_angle', 'point_source_id', 'gps_time', 'red', 'green', 'blue', 'PredSemantic', 'PredInstance', 'gt_semantic_segmentation', 'gt_instance_segmentation']\n",
      "['X', 'Y', 'Z', 'intensity', 'return_number', 'number_of_returns', 'synthetic', 'key_point', 'withheld', 'overlap', 'scanner_channel', 'scan_direction_flag', 'edge_of_flight_line', 'classification', 'user_data', 'scan_angle', 'point_source_id', 'gps_time', 'red', 'green', 'blue', 'PredSemantic', 'gt_semantic_segmentation', 'PredInstance']\n"
     ]
    }
   ],
   "source": [
    "gt_src = list(list_preds_gt.keys())[0]\n",
    "laz_gt = laspy.read(gt_src)\n",
    "# points_gt = np.vstack((gt.x, gt.y, gt.z)).transpose()\n",
    "\n",
    "pred_src = list_preds_gt[gt_src][-1]\n",
    "laz_pred = laspy.read(pred_src)\n",
    "# points_pred = np.vstack((pred.x, pred.y, pred.z)).transpose()\n",
    "# print(points_pred.shape)\n",
    "# print(points_gt.shape)\n",
    "print(list(laz_gt.point_format.dimension_names))\n",
    "print(list(laz_pred.point_format.dimension_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_instances = laz_gt.gt_instance_segmentation\n",
    "gt_semantic = laz_gt.gt_semantic_segmentation\n",
    "pred_instances = laz_pred.PredInstance\n",
    "pred_semantic = laz_pred.PredSemantic\n",
    "\n",
    "gt_instances_format, gt_semantic_format = get_segmentation(gt_instances, gt_semantic)\n",
    "pred_instances_format, pred_semantic_format = get_segmentation(pred_instances, pred_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (394889,) (388709,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m PQ, SQ, RQ \u001b[38;5;241m=\u001b[39m compute_panoptic_quality(gt_instances_format, pred_instances_format)\n\u001b[1;32m----> 2\u001b[0m mean_iou \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_mean_iou\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlaz_gt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgt_semantic_segmentation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlaz_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPredSemantic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPQ\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, SQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSQ\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, RQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRQ\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, mIoU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_iou\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 74\u001b[0m, in \u001b[0;36mcompute_mean_iou\u001b[1;34m(y_true, y_pred, num_classes)\u001b[0m\n\u001b[0;32m     71\u001b[0m iou_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_classes):\n\u001b[1;32m---> 74\u001b[0m     tp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     75\u001b[0m     fp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((y_true \u001b[38;5;241m!=\u001b[39m c) \u001b[38;5;241m&\u001b[39m (y_pred \u001b[38;5;241m==\u001b[39m c))\n\u001b[0;32m     76\u001b[0m     fn \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((y_true \u001b[38;5;241m==\u001b[39m c) \u001b[38;5;241m&\u001b[39m (y_pred \u001b[38;5;241m!=\u001b[39m c))\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (394889,) (388709,) "
     ]
    }
   ],
   "source": [
    "PQ, SQ, RQ = compute_panoptic_quality(gt_instances_format, pred_instances_format)\n",
    "mean_iou = compute_mean_iou(laz_gt.gt_semantic_segmentation, laz_pred.PredSemantic)\n",
    "print(f\"PQ: {PQ:.4f}, SQ: {SQ:.4f}, RQ: {RQ:.4f}, mIoU: {mean_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing on all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_pointclouds(laz1, laz2):\n",
    "    \"\"\"Sort laz2 to match the order of laz1 without changing laz1's order.\n",
    "\n",
    "    Args:\n",
    "        laz1: laspy.LasData object (reference order)\n",
    "        laz2: laspy.LasData object (to be sorted)\n",
    "    \n",
    "    Returns:\n",
    "        laz2 sorted to match laz1\n",
    "    \"\"\"\n",
    "    # Retrieve and round coordinates for robust matching\n",
    "    coords_1 = np.round(np.vstack((laz1.x, laz1.y, laz1.z)), 2).T\n",
    "    coords_2 = np.round(np.vstack((laz2.x, laz2.y, laz2.z)), 2).T\n",
    "\n",
    "    # Verify laz2 is a subset of laz1\n",
    "    assert len(coords_2) == len(coords_1), \"laz2 should be a subset of laz1\"\n",
    "\n",
    "    # Create a dictionary mapping from coordinates to indices\n",
    "    coord_to_idx = {tuple(coord): idx for idx, coord in enumerate(coords_1)}\n",
    "\n",
    "    # Find indices in laz1 that correspond to laz2\n",
    "    matching_indices = []\n",
    "    failed = 0\n",
    "    for coord in coords_2:\n",
    "        try:\n",
    "            matching_indices.append(coord_to_idx[tuple(coord)])\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "    # print(f\"Number of non-matching points: {failed}\")\n",
    "\n",
    "    matching_indices = np.array([coord_to_idx[tuple(coord)] for coord in coords_2])\n",
    "\n",
    "    # Sort laz2 to match laz1\n",
    "    sorted_indices = np.argsort(matching_indices)\n",
    "\n",
    "    # Apply sorting to all attributes of laz2\n",
    "    # for dim in laz2.point_format.dimensions:\n",
    "    #     setattr(laz2, dim.name, getattr(laz2, dim.name)[sorted_indices])\n",
    "    laz2.points = laz2.points[sorted_indices]\n",
    "\n",
    "    return laz2  # Now sorted to match laz1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [08:16<00:00, 82.71s/it] \n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pickle\n",
    "metrics = ['PQ', 'SQ', 'RQ', 'mIoU', 'Recall', 'Precision']\n",
    "metrics_res = {metric: np.zeros((len(samples_num),len(tilling_num))) for metric in metrics}\n",
    "\n",
    "for i, samp_num in tqdm(enumerate(samples_num), total=len(samples_num)):\n",
    "    # print(f\"PROCESSING SAMPLE {samp_num}\")\n",
    "    original_src = list_originals[samp_num]\n",
    "    gt_src = list_gt[samp_num]\n",
    "    laz_original = laspy.read(original_src)\n",
    "\n",
    "    for j, tilling in enumerate(tilling_num):\n",
    "        pred_src = [x for x in list_preds[samp_num] if len(x.split(f'{tilling}m')) > 1][0]\n",
    "        floor_src = [x for x in list_floors[samp_num] if len(x.split(f'{tilling}m')) > 1][0]\n",
    "        flatten_src = [x for x in list_flatten[samp_num] if len(x.split(f'{tilling}m')) > 1][0]\n",
    "        mask_src = [x for x in list_masks[samp_num] if len(x.split(f'{tilling}m')) > 1][0]\n",
    "\n",
    "        laz_pred = laspy.read(pred_src)\n",
    "        laz_gt = laspy.read(gt_src)\n",
    "        laz_floor = laspy.read(floor_src)\n",
    "        laz_flatten = laspy.read(flatten_src)\n",
    "\n",
    "        laz_pred = match_pointclouds(laz_flatten, laz_pred)\n",
    "\n",
    "        pred_coords = np.vstack((laz_pred.x, laz_pred.y, laz_pred.z)).T\n",
    "        gt_coords = np.vstack((laz_gt.x, laz_gt.y, laz_gt.z)).T\n",
    "        floor_coords = np.vstack((laz_floor.x, laz_floor.y, laz_floor.z)).T\n",
    "\n",
    "        remove_duplicates(laz_gt)\n",
    "        laz_gt = match_pointclouds(laz_original, laz_gt)\n",
    "\n",
    "        # Crop groud truth\n",
    "        if len(pred_src.split('flatten')) > 1:\n",
    "            # add floor to preds\n",
    "            pred_coords[:,2] = pred_coords[:,2] + floor_coords[:,2]\n",
    "            setattr(laz_pred, 'x', pred_coords[:,0])\n",
    "            setattr(laz_pred, 'y', pred_coords[:,1])\n",
    "            setattr(laz_pred, 'z', pred_coords[:,2])\n",
    "\n",
    "            # load mask\n",
    "            with open(mask_src, 'rb') as infile:\n",
    "                mask = pickle.load(infile)\n",
    "            laz_gt.points = laz_gt.points[mask]\n",
    "            laz_gt = match_pointclouds(laz_pred, laz_gt)\n",
    "\n",
    "        # Compute metrics\n",
    "        gt_instances = laz_gt.gt_instance_segmentation\n",
    "        gt_semantic = laz_gt.gt_semantic_segmentation\n",
    "        pred_instances = laz_pred.PredInstance\n",
    "        pred_semantic = laz_pred.PredSemantic\n",
    "\n",
    "        gt_instances_format, gt_semantic_format = get_segmentation(gt_instances, gt_semantic)\n",
    "        pred_instances_format, pred_semantic_format = get_segmentation(pred_instances, pred_semantic)\n",
    "        \n",
    "        PQ, SQ, RQ, tp, fp, fn = compute_panoptic_quality(gt_instances_format, pred_instances_format)\n",
    "        mean_iou = compute_mean_iou(gt_semantic, pred_semantic)\n",
    "        metrics_res['PQ'][i,j] = PQ\n",
    "        metrics_res['SQ'][i,j] = SQ\n",
    "        metrics_res['RQ'][i,j] = RQ\n",
    "        metrics_res['mIoU'][i,j] = mean_iou\n",
    "        metrics_res['Recall'][i,j] = round(tp/(tp + fn), 2) if tp + fn > 0 else 0\n",
    "        metrics_res['Precision'][i,j] = round(tp/(tp + fp),2) if tp + fn > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(metrics_res)\n",
    "# arr_metric_res = np.zeros((6*5, 4))\n",
    "# # for i in tqdm(range(arr_metric_res.shape[0]), total=arr_metric_res.shape[0]):\n",
    "# for i in range(arr_metric_res.shape[0]):\n",
    "#     # Load pointclouds\n",
    "#     gt_src = list(list_preds_gt.keys())[i//6]\n",
    "#     # pred_src = list_preds_gt[gt_src][i%5]\n",
    "#     # floor_src = list_floors_gt[gt_src][i%5]\n",
    "#     original_full_src = list_originals[i//6]\n",
    "#     pred_src = \"../data/flattening_corrections/test/color_grp_full_tile_128_flatten_10m_out.laz\"\n",
    "#     floor_src = \"../data/flattening_corrections/test/color_grp_full_tile_128_floor_10m.laz\"\n",
    "#     original_src = \"../data/flattening_corrections/test/color_grp_full_tile_128_flatten_10m.laz\"\n",
    "#     laz_pred = laspy.read(pred_src)\n",
    "#     laz_gt = laspy.read(gt_src)\n",
    "#     laz_floor = laspy.read(floor_src)\n",
    "#     laz_original = laspy.read(original_src)\n",
    "#     laz_original_full = laspy.read(original_full_src)\n",
    "\n",
    "#     laz_pred = match_pointclouds(laz_original, laz_pred)\n",
    "#     print(\"Original: \", original_src)\n",
    "#     print(\"GT: \", gt_src)\n",
    "#     print(\"Pred: \", pred_src)\n",
    "#     print(\"Floor: \", floor_src)\n",
    "#     pred_coords = np.vstack((laz_pred.x, laz_pred.y, laz_pred.z)).T\n",
    "#     gt_coords = np.vstack((laz_gt.x, laz_gt.y, laz_gt.z)).T\n",
    "#     floor_coords = np.vstack((laz_floor.x, laz_floor.y, laz_floor.z)).T\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#     remove_duplicates(laz_gt)\n",
    "#     laz_gt = match_pointclouds(laz_original_full, laz_gt)\n",
    "\n",
    "#     # laz_gt = match_pointclouds(laz_original, laz_gt)\n",
    "#     # for i in tqdm(range(len(laz_gt))):\n",
    "#     #     if not (laz_gt.xyz[i,:] == laz_original.xyz[i,:]).all(axis=0):\n",
    "#     #         print(\"YARGH\")\n",
    "#     print(f\"Is gt equal original : {np.array_equal(np.vstack(np.round((laz_gt.x, laz_gt.y, laz_gt.z),2)), np.vstack(np.round((laz_original.x, laz_original.y, laz_original.z),2)))}\")\n",
    "\n",
    "#     if round(laz_gt.z[748],2) != round(laz_original.z[748],2):\n",
    "#         print(\"YARGH\")\n",
    "#     # gt_coords = np.vstack((laz_gt.x, laz_gt.y, laz_gt.z)).T\n",
    "\n",
    "#     # Crop groud truth\n",
    "#     if len(pred_src.split('flatten')) > 1:\n",
    "#         # add floor to preds\n",
    "#         pred_coords[:,2] = pred_coords[:,2] + floor_coords[:,2]\n",
    "#         setattr(laz_pred, 'x', pred_coords[:,0])\n",
    "#         setattr(laz_pred, 'y', pred_coords[:,1])\n",
    "#         setattr(laz_pred, 'z', pred_coords[:,2])\n",
    "#         laz_pred.write('../data/test.laz')\n",
    "\n",
    "#         # load mask\n",
    "#         mask_src = list_masks_gt[gt_src][i%5]\n",
    "#         with open(mask_src, 'rb') as infile:\n",
    "#             mask = pickle.load(infile)\n",
    "#         laz_gt.points = laz_gt.points[mask]\n",
    "#         # laz_original.points = laz_original.points[mask]\n",
    "#         # laz_original = match_pointclouds(laz_pred, laz_original)\n",
    "#         laz_gt = match_pointclouds(laz_pred, laz_gt)\n",
    "#         print(laz_original.xyz[7648,:])\n",
    "#         print(len(laz_pred))\n",
    "#         print(len(laz_gt))\n",
    "#         # laz_gt = match_pointclouds(laz_pred, laz_gt)\n",
    "#         print(len(laz_gt))\n",
    "#         # laz_gt.xyz -= laz_floor.xyz\n",
    "#     # print(pred_coords[7648,:])\n",
    "#     print(laz_gt.xyz[7648,:])\n",
    "#     print(laz_pred.xyz[7648,:])\n",
    "#     # print(laz_pred.z[7648])\n",
    "#     # print(laz_floor.xyz[7648,:])\n",
    "#     print(f\"Is gt equal pred : {np.array_equal(np.vstack(np.round((laz_gt.x, laz_gt.y, laz_gt.z),2)), np.vstack(np.round((laz_pred.x, laz_pred.y, laz_pred.z),2)))}\")\n",
    "#     # print(f\"Is gt equal pred : {np.array_equal(np.round(laz_gt.xyz,2), np.round(laz_pred.xyz,2))}\")\n",
    "\n",
    "#     # Compute metrics\n",
    "#     gt_instances = laz_gt.gt_instance_segmentation\n",
    "#     gt_semantic = laz_gt.gt_semantic_segmentation\n",
    "#     pred_instances = laz_pred.PredInstance\n",
    "#     pred_semantic = laz_pred.PredSemantic\n",
    "\n",
    "#     gt_instances_format, gt_semantic_format = get_segmentation(gt_instances, gt_semantic)\n",
    "#     pred_instances_format, pred_semantic_format = get_segmentation(pred_instances, pred_semantic)\n",
    "\n",
    "#     # PQ, SQ, RQ = compute_panoptic_quality(gt_instances_format, pred_instances_format)\n",
    "#     # mean_iou = compute_mean_iou(laz_gt.gt_semantic_segmentation, laz_pred.PredSemantic)\n",
    "    \n",
    "#     PQ, SQ, RQ, tp, fp, fn = compute_panoptic_quality(gt_instances_format, pred_instances_format)\n",
    "#     mean_iou = compute_mean_iou(gt_semantic, pred_semantic)\n",
    "\n",
    "#     print(set(gt_instances))\n",
    "#     print(set(gt_semantic))\n",
    "#     print(set(pred_instances))\n",
    "#     print(set(pred_semantic))\n",
    "\n",
    "\n",
    "#     print(f\"PQ: {PQ:.4f}, SQ: {SQ:.4f}, RQ: {RQ:.4f}, tp: {tp}, fp: {fp}, fn: {fn}, Recall: {round(tp/(tp + fn), 2)}, Precision: {round(tp/(tp + fp),2)}, mIoU: {mean_iou:.4f}\")\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQ\n",
      "[[0.27893734 0.34397479 0.30623971 0.3653979 ]\n",
      " [0.01545159 0.00774913 0.0076665  0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.40451977 0.55216285 0.33103448 0.56080114]\n",
      " [0.48505885 0.54665073 0.54924323 0.50812442]\n",
      " [0.30352201 0.31660588 0.29101086 0.28828947]]\n",
      "SQ\n",
      "[[0.85424559 0.87904667 0.90034476 0.89944099]\n",
      " [0.64896697 0.68967276 0.69765143 0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.60677966 0.82824427 0.82758621 0.84120172]\n",
      " [0.87683715 0.87416992 0.88378229 0.86443117]\n",
      " [0.72727028 0.71456188 0.70768551 0.69629237]]\n",
      "RQ\n",
      "[[0.32653061 0.39130435 0.34013605 0.40625   ]\n",
      " [0.02380952 0.01123596 0.01098901 0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.66666667 0.66666667 0.4        0.66666667]\n",
      " [0.55319149 0.62533693 0.62146893 0.58781362]\n",
      " [0.41734417 0.44307692 0.41121495 0.41403509]]\n",
      "mIoU\n",
      "[[0.63191958 0.63392697 0.63583162 0.65339008]\n",
      " [0.35938154 0.3375126  0.32546136 0.3116106 ]\n",
      " [0.45848743 0.45757023 0.44668689 0.47684911]\n",
      " [0.59044894 0.59323686 0.60161599 0.60992501]\n",
      " [0.9062863  0.91023848 0.91236526 0.90500693]\n",
      " [0.67216512 0.60174681 0.57965267 0.55939648]]\n",
      "Recall\n",
      "[[0.67 0.84 0.78 0.84]\n",
      " [0.43 0.29 0.4  0.  ]\n",
      " [0.   0.   0.   0.  ]\n",
      " [1.   1.   1.   1.  ]\n",
      " [0.84 0.89 0.92 0.9 ]\n",
      " [0.52 0.49 0.49 0.47]]\n",
      "Precision\n",
      "[[0.22 0.25 0.22 0.27]\n",
      " [0.01 0.01 0.01 0.  ]\n",
      " [0.   0.   0.   0.  ]\n",
      " [0.5  0.5  0.25 0.5 ]\n",
      " [0.41 0.48 0.47 0.44]\n",
      " [0.35 0.4  0.36 0.37]]\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics_res.keys():\n",
    "    print(metric)\n",
    "    print(metrics_res[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\\flattening_corrections\\predictions\\color_grp_full_tile_128_flatten_1m_out.laz\n",
      "..\\data\\flattening_corrections\\floors\\color_grp_full_tile_128_floor_1m.laz\n",
      "..\\data\\flattening_corrections\\originals\\color_grp_full_tile_128_flatten_1m.laz\n",
      "..\\data\\flattening_corrections\\gt\\color_grp_full_tile_128_out_gt.laz\n",
      "392648\n",
      "388464\n",
      "388464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386579\n",
      "386579\n",
      "366718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "arr_metric_res = np.zeros((6*5, 4))\n",
    "for i in tqdm(range(arr_metric_res.shape[0]), total=arr_metric_res.shape[0]):\n",
    "    i=1\n",
    "    gt_src = list(list_preds_gt.keys())[i//5]\n",
    "    floor_src = list_floors[i//5]\n",
    "    if len(list_preds_gt[gt_src]) < i % 5:\n",
    "        continue\n",
    "    pred_src = list_preds_gt[gt_src][i%5]\n",
    "    floor_src = list_floors_gt[gt_src][i%5]\n",
    "    original_src = list_originals[gt_src][i+1%5]\n",
    "    # print(pred_src)\n",
    "    # print(floor_src)\n",
    "    # print(original_src)\n",
    "    # print(gt_src)\n",
    "    # load files and addapt gt to pred size\n",
    "    laz_gt = laspy.read(gt_src)\n",
    "    laz_pred = laspy.read(pred_src)\n",
    "    laz_floor = laspy.read(floor_src)\n",
    "    laz_original = laspy.read(original_src)\n",
    "\n",
    "    print(len(laz_pred))\n",
    "    print(len(laz_original))\n",
    "    print(len(laz_floor))\n",
    "    remove_duplicates(laz_pred)\n",
    "    remove_duplicates(laz_original)\n",
    "    remove_duplicates(laz_floor)\n",
    "    \n",
    "    print(len(laz_pred))\n",
    "    print(len(laz_original))\n",
    "    print(len(laz_floor))\n",
    "    break\n",
    "    with open(list_masks_gt[gt_src][(i-1)%5], 'rb') as file:\n",
    "        mask = pickle.load(file)\n",
    "\n",
    "    # load files and addapt gt to pred size\n",
    "    laz_gt = laspy.read(gt_src)\n",
    "    laz_pred = laspy.read(pred_src)\n",
    "    laz_floor = laspy.read(floor_src)\n",
    "    print(floor_src)\n",
    "\n",
    "    # Extract (x, y, z) coordinates as Nx3 arrays\n",
    "    coords_gt = np.round(np.vstack((laz_gt.x, laz_gt.y)),2).T\n",
    "    # coords_gt = coords_gt[mask]\n",
    "    coords_pred = np.round(np.vstack((laz_pred.x, laz_pred.y, laz_pred.z)),2).T\n",
    "    print(coords_gt.shape)\n",
    "    print(coords_pred.shape)\n",
    "\n",
    "    original = laspy.read(r\"..\\data\\flattening_corrections\\originals\\color_grp_full_tile_128_flatten_1m.laz\")\n",
    "    coords_original = np.round(np.vstack((original.x, original.y, original.z)),2).T\n",
    "    counter = 0\n",
    "    for row in tqdm(range(coords_original.shape[0]), total=coords_original.shape[0]):\n",
    "        matches = np.isin(coords_pred, coords_original[row,:]).all(axis=1)\n",
    "        counter += np.sum(matches)\n",
    "    print(counter)\n",
    "    break\n",
    "    print(\"original: \", len(original))\n",
    "    print(\"gt: \", len(laz_gt))\n",
    "    print(\"pred: \", len(laz_pred))\n",
    "    print(\"mask: \", len(mask))\n",
    "    print(\"floor: \", len(laz_floor))\n",
    "\n",
    "\n",
    "    tree_B = cKDTree(coords_pred)\n",
    "    print(\"computing distances...\")\n",
    "    distances, indices = tree_B.query(original, distance_upper_bound=1e-2, workers=-1)\n",
    "    print('done')\n",
    "    # pairs = tree_B.query_pairs(5e-2)\n",
    "    mask = distances != np.inf\n",
    "    coords_pred = coords_pred[mask]\n",
    "    # laz_gt = laz_gt.points[mask]\n",
    "    print(\"masked pred\", len(coords_pred))\n",
    "    # print(len(pairs))\n",
    "\n",
    "    break\n",
    "    # print(min(coords_gt[:,2]))\n",
    "    # print(min(coords_pred[:,2]))\n",
    "    # coords_pred[:,2] += np.min(coords_gt[:2])\n",
    "    # # Find the points in A that exist in B\n",
    "    # mask = np.isin(coords_gt.view([('', coords_gt.dtype)] * 3), \n",
    "    #             coords_pred.view([('', coords_pred.dtype)] * 3)).all(axis=1)\n",
    "\n",
    "    # # Filter A to keep only matching points\n",
    "    # laz_gt = laz_gt.points[mask]\n",
    "\n",
    "    tree_B = cKDTree(coords_pred)\n",
    "    distances, indices = tree_B.query(coords_gt, distance_upper_bound=1e-2)\n",
    "    mask = distances != np.inf\n",
    "    laz_gt = laz_gt.points[mask]\n",
    "\n",
    "    # Compute the metrics\n",
    "    gt_instances_format, gt_semantic_format = get_segmentation(laz_gt.gt_instance_segmentation, laz_pred.PredInstance)\n",
    "    PQ, SQ, RQ = compute_panoptic_quality(gt_instances_format, pred_instances_format)\n",
    "    mean_iou = compute_mean_iou(laz_gt.gt_semantic_segmentation, laz_pred.PredSemantic)\n",
    "\n",
    "    # save metrics\n",
    "    arr_metric_res[i,:] = np.array([PQ, SQ, RQ, mean_iou])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape:  388464\n",
      "original shape:  392648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 388464/388464 [1:20:18<00:00, 80.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of val repeated 8x: 204\n",
      "Number of val repeated 1x: 384697\n",
      "Number of val repeated 4x: 3554\n",
      "Number of val repeated 9x: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "laz_original = laspy.read(r\"..\\data\\flattening_corrections\\originals\\color_grp_full_tile_128_flatten_1m.laz\")\n",
    "laz_pred = laspy.read(r\"..\\data\\flattening_corrections\\predictions\\color_grp_full_tile_128_flatten_1m_out.laz\")\n",
    "coords_pred = np.round(np.vstack((laz_pred.x, laz_pred.y, laz_pred.z)),2).T\n",
    "coords_original = np.round(np.vstack((original.x, original.y, original.z)),2).T\n",
    "print(\"original shape: \", len(coords_original))\n",
    "print(\"original shape: \", len(coords_pred))\n",
    "\n",
    "counter = []\n",
    "nomatches = []\n",
    "multimatches = []\n",
    "for row in tqdm(range(coords_original.shape[0]), total=coords_original.shape[0]):\n",
    "    matches = np.isin(coords_pred, coords_original[row,:]).all(axis=1)\n",
    "    count = np.sum(matches)\n",
    "    counter.append(count)\n",
    "    if count == 0:\n",
    "        nomatches.append(row)\n",
    "    elif count > 1:\n",
    "        multimatches.append(row)\n",
    "    # if row == 1000:\n",
    "    #     break\n",
    "\n",
    "for val in set(counter):\n",
    "    print(f\"Number of val repeated {val}x: {len([x for x in counter if x == val])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = {\n",
    "    \"counter\": counter,\n",
    "    \"nomatches\": nomatches,\n",
    "    \"multi\": multimatches\n",
    "}\n",
    "with open(\"../data/flattening_corrections/duplicated_vals_128_1.pcl\", 'wb') as file:\n",
    "    pickle.dump(duplicated, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 386579/386579 [30:23<00:00, 212.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_same = 0\n",
    "non_matching_pred = []\n",
    "non_matching_original = []\n",
    "for row in tqdm(range(coords_pred.shape[0])):\n",
    "    # print(row)\n",
    "    # if np.round(coords_original[row,:], 2).all(axis=0) == np.round(coords_pred[row,:], 2).all(axis=0):\n",
    "    if np.any(np.all(coords_original == coords_pred[row,:], axis=1)):\n",
    "        count_same += 1\n",
    "    else:\n",
    "        non_matching_pred.append(np.round(coords_pred[row,:], 2))\n",
    "        non_matching_original.append(np.round(coords_original[row,:], 2))\n",
    "print(count_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A indices: [     0      1      2 ... 386576 386577 386578]\n",
      "B indices: [     0      1      2 ... 386576 386577 386578]\n"
     ]
    }
   ],
   "source": [
    "# Sort the two arrays\n",
    "\n",
    "# # Example arrays (shuffled versions of each other)\n",
    "# A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "# B = np.array([[7, 8, 9], [1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "A = coords_original\n",
    "B = coords_pred\n",
    "# Convert rows to tuples for sorting\n",
    "A_view = np.ascontiguousarray(A).view([('', A.dtype)] * 3)\n",
    "B_view = np.ascontiguousarray(B).view([('', B.dtype)] * 3)\n",
    "\n",
    "# Sort indices\n",
    "A_sort_idx = np.argsort(A_view.ravel())\n",
    "B_sort_idx = np.argsort(B_view.ravel())\n",
    "\n",
    "# Find corresponding indices\n",
    "A_sorted = A_view.ravel()[A_sort_idx]\n",
    "B_sorted = B_view.ravel()[B_sort_idx]\n",
    "\n",
    "# Get matching indices\n",
    "matching_B_idx = np.searchsorted(B_sorted, A_sorted)\n",
    "\n",
    "# Restore original indices\n",
    "matching_A_idx = A_sort_idx\n",
    "matching_B_idx = B_sort_idx[matching_B_idx]\n",
    "\n",
    "# Reorganise B\n",
    "coords_pred = coords_pred[matching_B_idx]\n",
    "coords_original = coords_original[matching_A_idx]\n",
    "\n",
    "# Print results\n",
    "print(\"A indices:\", matching_A_idx)\n",
    "print(\"B indices:\", matching_B_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/386579 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 386579/386579 [00:00<00:00, 433692.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test if both pointclouds matches\n",
    "for i in tqdm(range(coords_original.shape[0])):\n",
    "    if not (coords_original[i,:] == coords_pred[i,:]).all(axis=0):\n",
    "        print(\"YARGH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1478/32394 [01:12<25:26, 20.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other_row \u001b[38;5;129;01min\u001b[39;00m non_matching_original:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_row\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      7\u001b[0m         found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\PDM\\lib\\site-packages\\numpy\\core\\numeric.py:2439\u001b[0m, in \u001b[0;36marray_equal\u001b[1;34m(a1, a2, equal_nan)\u001b[0m\n\u001b[0;32m   2437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m equal_nan:\n\u001b[1;32m-> 2439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2440\u001b[0m \u001b[38;5;66;03m# Handling NaN values if equal_nan is True\u001b[39;00m\n\u001b[0;32m   2441\u001b[0m a1nan, a2nan \u001b[38;5;241m=\u001b[39m isnan(a1), isnan(a2)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count_diff = 0\n",
    "really_non_matching = []\n",
    "for _, row in tqdm(enumerate(non_matching_pred), total=len(non_matching_pred)):\n",
    "    found = False\n",
    "    for other_row in non_matching_original:\n",
    "        if np.array_equal(row, other_row):\n",
    "            found = True\n",
    "            break\n",
    "    if found == False:\n",
    "        count_diff += 1\n",
    "        really_non_matching.append(row)\n",
    "\n",
    "print(count_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take predictions out of the results folders\n",
    "from glob import glob\n",
    "src_root = r\"..\\data\\flattening_predictions_corrections\\predictions\"\n",
    "for filename in glob(os.path.join(src_root, '**/*.laz'), recursive=True):\n",
    "    os.replace(filename, os.path.join(src_root, os.path.basename(filename)))\n",
    "# for r, d, f in os.walk(src_root):\n",
    "#     print(r)\n",
    "#     print(d)\n",
    "#     print(f)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
